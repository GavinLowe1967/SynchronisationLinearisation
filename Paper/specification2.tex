\subsection{Variations}
\label{ssec:spec-variations}

Above we considered heterogeneous binary synchronisations,
i.e.~synchronisations between \emph{two} executions of \emph{different}
operations, with a single mode of synchronisation.

It is straightforward to generalise to synchronisations between an arbitrary
number of operation executions, some of which might be executions of the same
operation.  Consider a $k$-way synchronisation between operations
\begin{scala}
def op£\s j£(x£\s j£: A£\s j£): B£\s j£   £for $j = 1, \ldots, k,$£
\end{scala}
%
where the $\op_j$ might not be distinct.
The specification object will have a corresponding operation
%
\begin{scala} 
def sync(x£\s1£: A£\s1£, ..., x£\s k£: A£\s k£): (B£\s1£, ..., B£\s k£)
\end{scala}
%
For example, for the combining barrier |CombiningBarrier(n, f)| of the
Introduction, the corresponding specification object would be
% [A](n: Int, f: (A,A) => A)
\begin{scala}
class CombiningBarrierSpec{
  def sync(x£\s1£: A, ..., x£\s{\ss n}£: A) = {
    val result = f(x£\s1£, f(x£\s2£,...f(x£\s{{\ss{n}}-1}£, x£\s{\ss n}£)...)); (result,...,result)
  }
}
\end{scala}

A history of the specification object will have corresponding events
$\sm{sync}^{i_1, \ldots, i_k}(x_1, \ldots, x_k)\:: (y_1, \ldots, y_k)$.
%
The definition of synchronisation linearisation is an obvious adaptation of
the earlier definition: in the interleaving of the complete history of the
synchronisation history and the history of the specification object, each
$\sm{sync}^{i_1, \ldots, i_k}(x_1, \ldots, x_k)\:: (y_1, \ldots, y_k)$ occurs
between $\call.\op_1^{i_j}(x_j)$ and $\return.\op_j^{i_j}\::y_j$ for each $j =
1, \ldots, k$.
%% The definition of synchronisation-linearisability follows in the obvious
%% way.

Note that if several of the $\op_j$ are the same operation, there is a choice
as to the order in which their parameters are passed to |sync|.  (However, in
the case of the combining barrier, if |f| is associative and commutative, the
order makes no difference.)

It is also straightforward to adapt the definitions to deal with multiple
modes of synchronisation: the specification object has a different operation
for each mode.  For example, recall the |TimeoutChannel| from the
Introduction, where sends and receives may timeout and return without
synchronisation.  The corresponding specification object would be:
%
\begin{scala}
class TimeoutChannelSpec[A]{
  def sync£\s s£(x: A) = false       // £send£ times out.
  def sync£\s r£(u: Unit) = None  // £receive£ times out.
  def sync£\s{s,r}£(x: A, u: Unit) = (true, Some(x))  // Synchronisation.
}
\end{scala}
%
The operation $\sm{sync}_s$ corresponds to a send returning without
synchronising; likewise $\sm{sync}_r$ corresponds to a receive returning
without synchronising; and $\sm{sync}_{s,r}$ corresponds to a send and receive
synchronising.  The formal definition of synchronisation linearisation is the
obvious adaptation of the earlier definition: in particular |sync|\s{s} must
occur between the call and return of a send that doesn't synchronise, and
likewise for |sync|\s{r}.

As another example, the following is a specification object for a channel with
a close operation.
%
\begin{mysamepage}
\begin{scala}
class ClosableChannelSpec[A]{
  private var isClosed = false  // Is the channel closed? 
  def close(u: Unit) = { isClosed = true; () }
  def sync(x: A, u: Unit) = { require(!isClosed); ((), x) }
  def sendFail(x: A) = { require(isClosed); throw new Closed }
  def receiveFail(u: Unit) = { require(isClosed); throw new Closed }
}
\end{scala}
\end{mysamepage}
%
A send and receive can synchronise corresponding to |sync|, but only before
the channel is closed; or each may fail once the channel is closed,
corresponding to |sendFail| and |receiveFail|. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Specifying liveness}
\label{sec:progress}

We now consider a liveness condition for synchronisation objects.  
%% We assume that each pending operation execution is scheduled infinitely often,
%% unless it is blocked (for example, trying to obtain a lock); in other words,
%% the scheduler is fair to each execution.  Under this assumption, our liveness
The condition can be stated informally as:
%
\begin{itemize}
\item If some pending operation executions can synchronise (according to the
  synchronisation specification object), then some such synchronisation should
  happen;

\item Once a particular execution has synchronised, it should eventually
  return.
\end{itemize}
%
Several different synchronisations might be possible.  For example,
consider a synchronous channel, and suppose there are pending calls to
|send(4)|, |send(5)| and |receive|.  Then the |receive| could synchronise with
\emph{either} |send|, nondeterministically; subsequently, the |receive| should
return the appropriate value, and the corresponding |send| should also return.
In such cases, our liveness condition allows \emph{either} synchronisation to
occur.
%
However, our liveness condition allows all pending executions to block if no
synchronisation is possible.  For example, if every pending execution on a
synchronous channel is a |send|, then clearly none can make progress.

%% Note that our liveness condition is somewhat different from the condition of
%% \emph{lock freedom} for concurrent datatypes~\cite{herlihy-shavit}.  That
%% condition requires that, assuming operation executions collectively are
%% scheduled infinitely often, then eventually some execution returns.  Lock
%% freedom makes no assumption about scheduling being fair.  For example, if a
%% particular thread holds a lock, then lock freedom allows the scheduler to
%% never schedule that thread; in most cases, this will mean that no operation
%% returns: any implementation that uses a lock in a non-trivial way is not
%% lock-free.

%% By contrast, our assumption, that each unblocked pending execution is
%% scheduled infinitely often, reflects that modern schedulers \emph{are} fair,
%% and do not starve any single execution.  For example, if a thread holds
%% a lock,  and is not in a potentially unbounded loop or permanently blocked
%% trying to obtain a second lock, then it will be scheduled sufficiently often,
%% and so will eventually release the lock.  Thus our liveness condition can be
%% satisfied by an implementation that uses locks.  

%% However, our assumption does allow executions to be scheduled in an
%% unfortunate order (as long as each is scheduled infinitely often), which may
%% cause the liveness condition to fail.  It also allows other synchronisation
%% primitives, such as locks and semaphores, to be unfair: for example, a thread
%% that is repeatedly trying to obtain a lock may repeatedly fail as other
%% threads obtain the lock.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following definition identifies maximal sequences of synchronisations
that could occur given a particular history of a synchronisation object.
%
\begin{definition}
Given a history~$h$ of the synchronisation object and a legal history~$h_s$ of
the specification object, we say that $h_s$ is a \emph{maximal synchronisation
  linearisation} of~$h$ if:
\begin{itemize}
\item $h_s$ is a synchronisation linearisation of~$h$;

\item no proper legal extension $h_s \cat \seq{e}$ of~$h_s$ is a
  synchronisation linearisation of~$h$.
\end{itemize}
\end{definition}
%
% Note that a history may have multiple maximal synchronisation linearisations.
For example, the history 
\begin{eqnarray*}
h & = &\seq{\call.\send^1(4), \call.\send^2(5), \call.\receive^3(())}
\end{eqnarray*}
has two maximal synchronisation linearisations
\begin{eqnarray*}
h_s^1 & = &  \seq{\sync^{1,3}(4,())\::((),4)}, \\
h_s^2 & = & \seq{\sync^{2,3}(5,())\::((),5)},
\end{eqnarray*}
corresponding to the two possible synchronisations.  Each describes one
possibility for all the synchronisations that might happen. 

%%%%%

The following definition captures the $\return$ events that we would expect to
happen given a particular sequence of synchronisations.
%
\begin{definition}
Given a history~$h$ of the synchronisation object and a maximal
synchronisation linearisation~$h_s$, we say that a return event is
\emph{anticipated} if it does not appear in~$h$, but the corresponding $\sync$
event appears in~$h_s$.
\end{definition}
%
For example, considering the above histories~$h$ and~$h_s^1$, the events
$\return.\send^1\::()$ and $\return.\receive^3\::4$ are anticipated: assuming
$h_s^1$ describes the synchronisations that happen, we would expect those
$\return$ events to eventually happen; if they do not, that is a failure of
liveness.

%%%%%

%% The following definition captures our fairness assumption.
%% %
%% \begin{definition}
%% Given an execution, we say 
%% %% that an operation execution is \emph{pending} if it
%% %% has been called but not yet returned.  We say 
%% that an operation execution is \emph{blocked} in a particular state if it is
%% unable to perform a step, for example because it is trying to acquire a lock
%% held by another thread, or waiting to receive a signal from another thread.

%% We say that an infinite execution is \emph{fair} if each pending operation
%% execution either (a)~eventually returns, (b)~is blocked in infinitely many
%% states, or (c)~performs infinitely many steps.  We consider a system execution
%% that reaches a deadlocked state (where every pending operation is permanently
%% blocked) to be infinite (it is infinite in time), and hence fair (under~(b)).
%% \end{definition}

%% The main reason we require fairness is to rule out executions where a thread
%% obtains a lock, but then is permanently descheduled, which could permanently
%% block other threads.  Modern schedulers are fair in this sense.

%%%%%

\begin{definition}
Let $h$ be a history of the synchronisation object.  We say that $h$ is
\emph{synchronisation progressible} if for every 
% fair infinite
execution following~$h$ with no new $\call$ events, there is a
maximal synchronisation linearisation~$h_s$ of~$h$ such that every anticipated
return event~$ret$ eventually happens.
%
We say that a synchronisation object is synchronisation progressible if all of
its histories are synchronisation progressible.
\end{definition}

%% Given a history~$h$ of the synchronisation object and a maximal
%% synchronisation linearisation~$h_s$, we say that $h$ is \emph{synchronisation
%%   progressible} with respect to~$h_s$ if for every anticipated return
%% event~$ret$, and for every fair infinite execution with no new $\call$ events,
%% $ret$ eventually happens.

%% We say that $h$ is \emph{synchronisation progressible} is there is some
%% maximal synchronisation linearisation~$h_s$ such that $h$ is synchronisation
%% progressible with respect to~$h_s$.
%% \end{definition}

For the earlier history~$h$, synchronisation progressibility
requires that either $\return.\send^1\::()$ and $\return.\receive^3\::4$
eventually happen (corresponding to~$h_s^1$), or $\return.\send^2\::()$ and
$\return.\receive^3\::5$ eventually happen (corresponding to~$h_s^2$): one of
the synchronisations should happen, and then the relevant operations should
return. 

On the other hand, for the history $\seq{\call.\send^1(4)}$, the only
maximal synchronisation linearisation is $\seq{}$, for which there are no
anticipated returns, and so synchronisation progressibility is trivially
satisfied: it is fine for the $\send$ to get stuck in this case, since there
is no $\receive$ with which it could synchronise.

Any claim of synchronisation progressibility necessarily makes 
assumptions about the way scheduling is performed.  Possible assumptions are
as follows.
%
\begin{enumerate}
\item\label{assump:some} The minimal possible assumption is that the scheduler
  schedules \emph{some} thread whenever there is a thread that can take a
  step.  Clearly, if no thread is scheduled, none will return.  Any reasonable
  scheduler satisfies this property.

\item\label{assump:spurious} Several concurrency primitives allow a thread to
  suspend and wait for a signal.  However, some (e.g.~monitors in the JVM)
  allow waiting threads to wake up and resume without a corresponding signal,
  so called \emph{spurious wakeups}.  Code that uses such primitives are
  expected to guard against spurious wakeups, typically by checking an
  appropriate condition, and waiting again if it doesn't hold.  When analysing
  such code for progressibility, we need to assume that spurious wakeups do
  not happen so often that that no other thread is scheduled.  In other words,
  we assume there is no infinite execution where one or more threads
  repeatedly perform a spurious wakeup, check the condition, and wait again,
  without any other threads being scheduled.  Spurious wakeups happen rarely
  in practice, so this is a very reasonable assumption.

\item\label{assump:fair} Some synchronisation objects may require an
  assumption that scheduling is \emph{fair} between threads.  For example,
  consider an object that uses a spin-lock~\cite[Chapter 7]{herlihy-shavit},
  where a thread that is trying to acquire the lock spins, repeatedly checking
  some condition, until no other thread holds the lock.  Consider a situation
  where thread~$A$ is holding the lock, and thread~$B$ is spinning, waiting to
  acquire the lock.  In this situation, we will need to assume that thread~$A$
  is scheduled sufficiently often, so that it eventually releases the lock.
  In other words, we assume there is no infinite execution where only
  thread~$B$ is scheduled.  Most modern schedulers are fair in this sense.

\item Some synchronisation objects may require synchronisation primitives,
  such as locks and semaphores, to be fair.  Such an assumption might require
  that there is no execution where one thread repeatedly fails to obtain a
  lock, while other threads obtain the lock infinitely often.
\end{enumerate}
%
In the examples we consider in Section~\ref{sec:experiments}, the
implementations require assumptions~\ref{assump:some}
and~\ref{assump:spurious} to satisfy synchronisation progressibility. 

Our liveness condition is somewhat different from the condition of \emph{lock
  freedom} for concurrent datatypes~\cite{herlihy-shavit}.  That condition
requires that whenever there are pending operation executions, eventually some
such execution returns.  However, this condition treats a thread that is
blocked, trying to obtain a lock (whether a spin lock or not), as performing
infinitely many steps.  Hence any object that uses a lock in a non-trivial way
is not lock-free.

%% , assuming operation executions collectively are
%% scheduled infinitely often, then eventually some execution returns.  
%% In particular, it would not be satisfied by 

%%  Lock
%% freedom makes no assumption about scheduling being fair.  For example, if a
%% particular thread holds a lock, then lock freedom allows the scheduler to
%% never schedule that thread; in most cases, this will mean that no operation
%% returns: any implementation that uses a lock in a non-trivial way is not
%% lock-free.

By contrast, if a thread suspends, waiting to acquire a non-spin lock, we do
not consider it to be taking steps.  In most cases,
assumption~\ref{assump:some} will mean that the thread holding the lock will
be scheduled sufficiently many times that it eventually releases the lock.
Thus such an object can satisfy synchronisation progressibility.  A similar
argument holds for a spin lock, but requires assumption~\ref{assump:fair}. 

%% By contrast, our assumption, that each unblocked pending execution is
%% scheduled infinitely often, reflects that modern schedulers \emph{are} fair,
%% and do not starve any single execution.  For example, if a thread holds
%% a lock,  and is not in a potentially unbounded loop or permanently blocked
%% trying to obtain a second lock, then it will be scheduled sufficiently often,
%% and so will eventually release the lock.  Thus our liveness condition can be
%% satisfied by an implementation that uses locks.  

%% However, our assumption does allow executions to be scheduled in an
%% unfortunate order (as long as each is scheduled infinitely often), which may
%% cause the liveness condition to fail.  It also allows other synchronisation
%% primitives, such as locks and semaphores, to be unfair: for example, a thread
%% that is repeatedly trying to obtain a lock may repeatedly fail as other
%% threads obtain the lock.
